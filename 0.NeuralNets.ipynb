{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04447bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "venv_path = \"venv\" # Name of the virtual environment\n",
    "\n",
    "if os.path.exists(venv_path): # Check if the Virtual Environment exists\n",
    "    print(\"venv exists\")\n",
    "else:\n",
    "    print(\"venv does not exist, creating it...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"venv\", venv_path]) # Creates venv for in the current script location\n",
    "\n",
    "# Detect OS\n",
    "system = platform.system()\n",
    "\n",
    "# Path to Python interpreter inside venv\n",
    "if system == \"Windows\":\n",
    "    python_bin = os.path.join(venv_path, \"Scripts\", \"python.exe\") # Python Interpreter location, for windows\n",
    "else:\n",
    "    python_bin = os.path.join(venv_path, \"bin\", \"python\") # Python Interpreter on linux\n",
    "\n",
    "# Show Python version\n",
    "subprocess.run([python_bin, \"--version\"])\n",
    "\n",
    "# Install packages using the venv's Python\n",
    "subprocess.run([python_bin, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f30cde",
   "metadata": {},
   "source": [
    "# Neural Networks - A recap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9a79e",
   "metadata": {},
   "source": [
    "### Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257969",
   "metadata": {},
   "source": [
    "- What are Neurons here?\n",
    "    - Models of neurons in our brain\n",
    "    - Consists of nodes of computations that are connected to other nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4efda0",
   "metadata": {},
   "source": [
    "### Perceptrons  \n",
    "Perceptrons are simple units that when exposed to input, outputs one or zero. The threshold is something you can choose.\n",
    "\n",
    "$$\n",
    "\\text{output} = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } \\sum_{j} w_{j} x_{j} + b \\leq 0, \\\\\n",
    "1 & \\text{if }\\sum_{j} w_{j} x_{j} + b > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $w$ is weight of the connection, i.e. importance of the connections. And corresponding $x$ is the input. and b is the bias term.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Perceptron.png\" alt=\"Perceptron Diagram\" style=\"width:400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ec359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  ##? A Library for fast linear algebra\n",
    "import matplotlib.pyplot as plt     ##? A standard ploting library, for quick ploting. For publication quality plots use plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22164641",
   "metadata": {},
   "outputs": [],
   "source": [
    " #* Perceptron In code. \n",
    "def perceptron_np(input: np.ndarray, weights: np.ndarray, bias: float):\n",
    "    sum = np.array(())\n",
    "    for j in range(len(input)):                     #? Loop through all the elements of the array\n",
    "        sum_j = (input[j] * weights[j])             #? Calculates the sum of j-th element\n",
    "        sum = np.append(sum, sum_j)                 #? Append the sum_j to the sum array\n",
    "    sum = np.sum(sum)                               #? Sum all the elements in sum array    \n",
    "    activation = 0 if sum + bias <= 0 else 1        #? condition for activation to be 1 or 0\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23986abe",
   "metadata": {},
   "outputs": [],
   "source": [
    " #* Perceptron In code. More optimized\n",
    "def perceptron_opt(input: np.ndarray, weights: np.ndarray, bias: float):\n",
    "    \"\"\"\n",
    "    A simple perceptron neurons\n",
    "    Arguments:\n",
    "        signal: A numpy array of input signal for perceptron.\n",
    "        weights: weights for corresponding input signal\n",
    "        bias: threshold for the perceptron\n",
    "    Returns:\n",
    "        activation of perceptron, integer - 0 or 1\n",
    "    \"\"\"\n",
    "    activation = np.sum(np.multiply(input, weights)) + bias     #? multiply does the same thing as (signal * weights) np.multiply makes it more readable. For matrix multiplication -> np.matmul(a,b)\n",
    "    return int(activation > 0)                                  #? output > 0 returns True if the statement is true and False otherwise, int convert it to 1 or 0 for True and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.random.random(3)    #? Generates a random array of length 3 between range [0, 1)\n",
    "signal = signal *2 -1           #? Push the range to [-1, 1)\n",
    "weights = np.random.random(3)\n",
    "weights = weights *2 -1\n",
    "bias = 0.6\n",
    "\n",
    "print(f\"Weights: {weights}\\nSignal: {signal}\\nBias: {bias}\\n\")\n",
    "print(f\"Activation from perceptron_np: {perceptron_np(signal, weights, bias)}\")\n",
    "print(f\"Activation from perceptron_opt: {perceptron_opt(signal, weights, bias)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2381308",
   "metadata": {},
   "source": [
    "#### Mini Exercise\n",
    "- Write perceptron with pytorch tensors.\n",
    "- remember type hinting and Commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    " #! Your solution\n",
    "def perceptron_torch():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47bf18",
   "metadata": {},
   "source": [
    "A perceptron is a neuron with step function as activation.\n",
    "Formally, Heaviside Step function is\n",
    "$$\n",
    "\\mathbf{H} = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } \\mathbf{Z_{j}} \\leq 0, \\\\\n",
    "1 & \\text{if } \\mathbf{Z_{j}} > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{Z}$ is weighted input for the neurons, for us $\\mathbf{Z(x)} = w_{j}x_{j} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    return np.where(x >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_functions #? plot_functions plots callable functions. compatible with torch.Tensors and np.ndarray. Can handle raw np.ndarrays and tensors as well.\n",
    "\n",
    "plt.show(plot_functions(step, title=\"Heaviside Function\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a050a4",
   "metadata": {},
   "source": [
    "Perceptrons are useful. For example in ref.2 they have used linked perceptrons to generate gates (NAND, AND, NOR and so on). \n",
    "But they can be limiting because they are not continuous and hance not differentiable. \n",
    "There are many possible activation functions that one can use. They are generally task dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ce1f2",
   "metadata": {},
   "source": [
    "### Sigmoid Neurons\n",
    "\n",
    "Sigmoid neurons are activated by sigmoid function,\n",
    "\n",
    "$$\n",
    "\\sigma(Z) = \\frac{1}{1 + e^{-Z}}\n",
    "$$\n",
    "\n",
    "writing everything explicitly for completeness,\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp\\left(-\\sum_{j} w_{j}x_{j} + b\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_j$ are the input signal\n",
    "- $w_j$ are the weights\n",
    "- $b$ is the bias term\n",
    "- $\\sum_{j} w_{j}x_{j} + b$ is the weighted sum $\\mathbf{Z}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Returns the activation from the weighted input\n",
    "    Arguments:\n",
    "        Z : weighted input\n",
    "    Returns:\n",
    "        activation of the neuron\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "ax = plot_functions(sigmoid, title=\"Sigmoid\")\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73875682",
   "metadata": {},
   "source": [
    "Sigmoids are good not only because they are differentiable but they also give us the advantage of tweaking the $\\mathbf{Z}$ slightly to have output change slightly, not dramatically like heaviside step function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652bd68e",
   "metadata": {},
   "source": [
    "#### Mini Exercise\n",
    "- Generate a weighted input from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #! Your Code\n",
    "def weighted_input(input):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ebd41",
   "metadata": {},
   "source": [
    "### Other Activations\n",
    "| Function      | Definition                                      | Range       | Differentiable? | Common Uses         |\n",
    "|--------------|------------------------------------------------|-------------|-----------------|---------------------|\n",
    "| **ReLU**     | $\\max(0, x)$                                   | $[0, ∞)$    | Yes* (at x≠0)   | Hidden layers       |\n",
    "| **Leaky ReLU** | $\\begin{cases} x & x \\geq 0 \\\\ 0.01x & x < 0 \\end{cases}$ | $(-∞, ∞)$ | Yes* (at x≠0)   | Fixes \"dying ReLU\"  |\n",
    "| **Tanh**     | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$            | $(-1, 1)$   | Yes             | RNNs/Hidden layers |\n",
    "| **Softmax**  | $\\frac{e^{x_i}}{\\sum e^{x_j}}$                | $(0, 1)$    | Yes             | Output layers       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "plt.show(plot_functions(F.relu, title= \"ReLU\"))     #? Change the name here to see the plot of the desired function. Softmax will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e827cd",
   "metadata": {},
   "source": [
    "## Architecture of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d38c1",
   "metadata": {},
   "source": [
    "Now that we have neurons, units that can compute output with the input after some transformation We move on to the network part.\n",
    "For complex tasks we would need more than one neuron. In a fashion where one's output is feed into the input of the next, and so on. A general Schamatic is:\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"NeuralNetwork.png\" alt=\"Perceptron Diagram\" style=\"width:500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8e5e0",
   "metadata": {},
   "source": [
    "This is fully connected Feed Forward architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network in code\n",
    "def feedforward_network(input_vector, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    # Input to hidden layer\n",
    "    hidden_input = np.dot(weights_hidden, input_vector) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    # Hidden to output layer\n",
    "    output_input = np.dot(weights_output, hidden_output) + bias_output\n",
    "    output = sigmoid(output_input)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example parameters\n",
    "np.random.seed(42)  # for reproducible results\n",
    "\n",
    "# 3 inputs, 5 hidden neurons\n",
    "weights_hidden = np.random.randn(5, 3)\n",
    "bias_hidden = np.random.randn(5)\n",
    "\n",
    "# 5 hidden neurons, 2 output neurons\n",
    "weights_output = np.random.randn(2, 5)\n",
    "bias_output = np.random.randn(2)\n",
    "\n",
    "# Example input vector (3 features)\n",
    "input_vector = np.array([0.7, 0.2, 0.9])\n",
    "\n",
    "# Forward pass\n",
    "output = feedforward_network(input_vector, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "print(\"Output of the network:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output Weights: {weights_output.shape}\\nHidden Weights: {weights_hidden.shape}\\nInput Vector: {input_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee4083e",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. **3Blue1Brown** - First 4 videos for intuitive understanding - https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=_R5V5O0lDB153wcA\n",
    "2. **Book** - Neural Networks and Deep Learning by Michael Nielsen - http://neuralnetworksanddeeplearning.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
